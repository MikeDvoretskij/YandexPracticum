{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba064640",
   "metadata": {
    "id": "ba064640",
    "outputId": "866f6d6f-4043-40a9-9803-f5360f0b6c6a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90cae8c",
   "metadata": {
    "id": "b90cae8c"
   },
   "outputs": [],
   "source": [
    "PATH = 'C:\\\\Users\\\\Rog\\\\Desktop\\\\Programing\\\\Yandex\\\\'\n",
    "WORD_TO_BLOCK = ['child', 'boy', 'girl', 'baby', 'teen', 'teenager', 'kid', 'infant', 'youngster', 'kids', 'children', 'boys', 'girls', 'babies', 'teens', 'teenagers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be7c69a",
   "metadata": {
    "id": "8be7c69a"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join(PATH, 'to_upload/train_dataset.csv'))\n",
    "train_data.name = 'train_data'\n",
    "data_crowd = pd.read_csv(os.path.join(PATH, 'to_upload/CrowdAnnotations.tsv'), sep='\\t', names = ['image', 'query_id', 'fraction', 'pros', 'cons'])\n",
    "data_crowd.name = 'data_crowd'\n",
    "data_expert = pd.read_csv(os.path.join(PATH, 'to_upload/ExpertAnnotations.tsv'), sep='\\t', names = ['image', 'query_id', 'first', 'second', 'third'])\n",
    "data_expert.name = 'data_expert'\n",
    "test_query = pd.read_csv(os.path.join(PATH, 'to_upload/test_queries.csv'), index_col=[0], sep='|')\n",
    "test_query.name = 'test_query'\n",
    "test_image = pd.read_csv(os.path.join(PATH, 'to_upload/test_images.csv'))\n",
    "test_image.name = 'test_image'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f711e957",
   "metadata": {
    "id": "f711e957",
    "outputId": "daa0307b-ceb0-4269-a25a-2dcb54a5dea7"
   },
   "outputs": [],
   "source": [
    "for dataframe in [train_data, data_crowd, data_expert, test_query, test_image]:\n",
    "    print('------------------------------------------------------------------------------------------------------------')\n",
    "    print('+-------------+')\n",
    "    print(f'| {dataframe.name}  |')\n",
    "    print('+-------------+')\n",
    "    print(dataframe.shape)\n",
    "    display(dataframe.head(5))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861df67d",
   "metadata": {
    "id": "861df67d",
    "outputId": "0f38f6ef-bfd1-48ec-e0d3-a0e14581eb06",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "samples_train = list(train_data['image'].sample(8))\n",
    "samples_test = list(test_query['image'].sample(8))\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "for i in range(8):\n",
    "    fig.add_subplot(4, 4, i+1)\n",
    "    image = Image.open(Path(PATH, 'to_upload/train_images', samples_train[i]))\n",
    "    plt.imshow(image)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()\n",
    "for i in range(8):\n",
    "    fig.add_subplot(4, 4, i+9)\n",
    "    image = Image.open(Path(PATH, 'to_upload/test_images', samples_test[i]))\n",
    "    plt.imshow(image)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b89e11",
   "metadata": {
    "id": "e7b89e11",
    "outputId": "e85f46c3-03e8-4702-9e94-d3cf06bc7818"
   },
   "outputs": [],
   "source": [
    "print(f\"Количество уникальных изображений в тренировочной выборке: {len(train_data.image.unique())}\")\n",
    "print(f\"Количество уникальных зфпросов в тренировочной выборке: {len(train_data.query_text.unique())}\")\n",
    "print(f\"Количество уникальных изображений в тестовой выборке: {len(test_image.image.unique())}\")\n",
    "print(f\"Количество уникальных зфпросов в тестовой выборке: {len(test_query.query_text.unique())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8e3a70",
   "metadata": {
    "id": "9e8e3a70",
    "outputId": "392e2dc0-49ab-4920-b918-1736e2c68e2c"
   },
   "outputs": [],
   "source": [
    "crowd_analicit = data_crowd.copy()\n",
    "crowd_analicit['fraction'] *= 100\n",
    "crowd_analicit['fraction'] = crowd_analicit['fraction'].astype('int')\n",
    "crowd_array=[]\n",
    "fraction_nums = np.array([0, 33, 66, 100])\n",
    "for i in fraction_nums:\n",
    "    crowd_array.append((crowd_analicit['fraction'] == i).sum())\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x = fraction_nums, y = crowd_array)\n",
    "plt.xticks(rotation=75, size=12)\n",
    "plt.xlabel('Доля людей, подтвердивших, что описание соответствует изображению (%)')\n",
    "plt.ylabel('Количество')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dd28b7",
   "metadata": {
    "id": "a6dd28b7",
    "outputId": "d52c8c5c-c04c-477c-9307-0e9480ed9a84"
   },
   "outputs": [],
   "source": [
    "def transform_range(x):\n",
    "    return round((x - 1) / 3.0, 2)\n",
    "\n",
    "def agr_data(row):\n",
    "    if row['first'] != row['second'] != row['third']:\n",
    "        row['agr_expert'] = (row['first'] + row['second'] + row['third']) // 3\n",
    "    else:\n",
    "        row['agr_expert'] = int(np.median(row['first':'third']))\n",
    "\n",
    "    return row\n",
    "\n",
    "data_expert = data_expert.apply(agr_data, axis=1)\n",
    "data_expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0b89b6",
   "metadata": {
    "id": "6d0b89b6",
    "outputId": "06998204-3f41-4fea-f23e-5d8ca686603f"
   },
   "outputs": [],
   "source": [
    "data_merged = data_expert.merge(data_crowd, on = ['image', 'query_id'], how='outer')\n",
    "data_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f2fbcf",
   "metadata": {
    "id": "e7f2fbcf",
    "outputId": "5f455c02-9d2a-4e76-ba71-08e3e8f4c5b7"
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('Количество NaN значений после объеденений массивов в agr_expert:', data_merged.isna().sum()[2])\n",
    "print('Количество NaN значений после объеденений массивов в fraction:', data_merged.isna().sum()[3])\n",
    "print()\n",
    "print('Матрица корреляции agr_expert и fraction:')\n",
    "display(data_merged[['agr_expert', 'fraction']].corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e20492",
   "metadata": {
    "id": "14e20492",
    "outputId": "fd1206b4-7f0e-4d77-e152-5e39032e27d1"
   },
   "outputs": [],
   "source": [
    "agr_expert_unique = data_merged.agr_expert.unique()\n",
    "agr_expert_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daeadf2",
   "metadata": {
    "id": "7daeadf2",
    "outputId": "643f9855-dcd0-4746-d0df-68119e03862c"
   },
   "outputs": [],
   "source": [
    "data_merged_notnan = data_merged.copy()\n",
    "data_merged_notnan = data_merged_notnan.dropna()\n",
    "\n",
    "for i in agr_expert_unique:\n",
    "    print(f'Средняя доля людей, подтвердивших, что описание соответствует изображению (%) при оценке экспертов равной {i}:',\n",
    "          '{:.0%}'.format(data_merged_notnan.loc[data_merged_notnan['agr_expert'] == i, 'fraction'].mean()),\n",
    "          ', std:',\n",
    "          '{:.0%}'.format(data_merged_notnan.loc[data_merged_notnan['agr_expert'] == i, 'fraction'].std()))\n",
    "\n",
    "print()\n",
    "print('Подсчёт уникальных значений столбца agr_expert')\n",
    "print(data_merged_notnan.agr_expert.value_counts())\n",
    "print()\n",
    "print('Подсчёт уникальных значений столбца fraction')\n",
    "print(data_merged_notnan.fraction.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be89040",
   "metadata": {
    "id": "8be89040"
   },
   "outputs": [],
   "source": [
    "def agr_merge(row):\n",
    "    if row['fraction'] >= 0.75:\n",
    "        return 4.0\n",
    "    elif row['fraction'] >= 0.25:\n",
    "        return 3.0\n",
    "    elif row['fraction'] >= 0.10:\n",
    "        return 2.0\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2798bb32",
   "metadata": {
    "id": "2798bb32",
    "outputId": "314bc30d-bf8e-4b87-8fc6-8936610f984d"
   },
   "outputs": [],
   "source": [
    "features_func = data_merged[['agr_expert', 'fraction']].copy()\n",
    "features_func.dropna(inplace=True)\n",
    "features_func['predict'] = features_func.apply(agr_merge, axis=1)\n",
    "\n",
    "agr_expert_models = pd.DataFrame(columns = ['ACC'])\n",
    "\n",
    "def alter_table(df, model_name, acc):\n",
    "    '''Добавляет в нашу таблицу с анализом моделей данные'''\n",
    "\n",
    "    agr_expert_models.loc[model_name, 'ACC'] = acc\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "alter_table(agr_expert_models,\n",
    "            'BaseLine',\n",
    "            round(accuracy_score(features_func['predict'], features_func['agr_expert']), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adb5ac9",
   "metadata": {
    "id": "8adb5ac9",
    "outputId": "7cc219ec-9446-4f69-d4d0-ebba426fedb1"
   },
   "outputs": [],
   "source": [
    "data_merged_expert = data_merged.copy()\n",
    "data_merged_expert.dropna(inplace=True)\n",
    "\n",
    "features = data_merged_expert[['fraction', 'pros', 'cons']]\n",
    "target = data_merged_expert.agr_expert\n",
    "\n",
    "print(features.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e35237",
   "metadata": {
    "id": "87e35237"
   },
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(features,\n",
    "                                                                            target,\n",
    "                                                                            test_size=0.25,\n",
    "                                                                            random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134515d3",
   "metadata": {
    "id": "134515d3",
    "outputId": "3503314c-ee65-4041-cf29-a8e205df46b6"
   },
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression(class_weight=\"balanced\", random_state=12345).fit(features_train, target_train)\n",
    "predictions = model_lr.predict(features_test)\n",
    "\n",
    "alter_table(agr_expert_models,\n",
    "            'LogisticRegression',\n",
    "            round(accuracy_score(predictions, target_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc2c914",
   "metadata": {
    "id": "cfc2c914",
    "outputId": "3ee77b1b-132a-46be-f998-2090c2c432b2"
   },
   "outputs": [],
   "source": [
    "model_svc = SVC(class_weight=\"balanced\", kernel = 'sigmoid').fit(features_train, target_train)\n",
    "predictions = model_svc.predict(features_test)\n",
    "\n",
    "alter_table(agr_expert_models,\n",
    "            'SVC',\n",
    "            round(accuracy_score(predictions, target_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea653fb",
   "metadata": {
    "id": "6ea653fb",
    "outputId": "0f8e71d4-0bc2-4dbe-8602-57d7f0d46e59"
   },
   "outputs": [],
   "source": [
    "model_rfc = RandomForestClassifier(class_weight=\"balanced\").fit(features_train, target_train)\n",
    "predictions = model_rfc.predict(features_test)\n",
    "\n",
    "alter_table(agr_expert_models,\n",
    "            'RandomForestClassifier',\n",
    "            round(accuracy_score(predictions, target_test), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53926880",
   "metadata": {
    "id": "53926880",
    "outputId": "3af83305-d9dc-4906-ff45-b0a46072f045"
   },
   "outputs": [],
   "source": [
    "agr_expert_models.sort_values(by='ACC', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a663ea",
   "metadata": {
    "id": "f4a663ea"
   },
   "outputs": [],
   "source": [
    "fraction_models = pd.DataFrame(columns = ['RMSE'])\n",
    "\n",
    "def alter_table(df, model_name, rmse):\n",
    "    fraction_models.loc[model_name, 'RMSE'] = rmse\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1bb6d8",
   "metadata": {
    "id": "7c1bb6d8",
    "outputId": "607a9d01-2ece-4be0-efe3-49593fdf429d"
   },
   "outputs": [],
   "source": [
    "features = data_merged_expert[['first', 'second', 'third', 'agr_expert']]\n",
    "target = data_merged_expert.fraction\n",
    "\n",
    "print(features.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02730594",
   "metadata": {
    "id": "02730594"
   },
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(features,\n",
    "                                                                            target,\n",
    "                                                                            test_size=0.25,\n",
    "                                                                            random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1042c",
   "metadata": {
    "id": "02d1042c",
    "outputId": "90ceac77-54b6-4ea0-fd26-cabb1a0fa2ef"
   },
   "outputs": [],
   "source": [
    "model_lr = LinearRegression().fit(features_train, target_train)\n",
    "predictions = model_lr.predict(features_test)\n",
    "\n",
    "alter_table(alter_table,\n",
    "            'LinearRegression',\n",
    "            round(np.sqrt(mean_squared_error(target_test, predictions)), 4))\n",
    "fraction_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e722c46f",
   "metadata": {
    "id": "e722c46f",
    "outputId": "095409f2-f024-44d4-f349-8b6f811e7418"
   },
   "outputs": [],
   "source": [
    "model_r = Ridge().fit(features_train, target_train)\n",
    "predictions = model_r.predict(features_test)\n",
    "\n",
    "alter_table(alter_table,\n",
    "            'Ridge',\n",
    "            round(np.sqrt(mean_squared_error(target_test, predictions)), 4))\n",
    "fraction_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948c805e",
   "metadata": {
    "id": "948c805e",
    "outputId": "5ecd5e44-c2bc-49cd-c014-e297da4d9fe0"
   },
   "outputs": [],
   "source": [
    "model_rfr = RandomForestRegressor().fit(features_train, target_train)\n",
    "predictions = model_rfr.predict(features_test)\n",
    "\n",
    "alter_table(alter_table,\n",
    "            'RandomForestRegressor',\n",
    "            round(np.sqrt(mean_squared_error(target_test, predictions)), 4))\n",
    "fraction_models.sort_values(by='RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5441ef18",
   "metadata": {
    "id": "5441ef18",
    "outputId": "6bd80d40-b802-42b2-8f77-632bd842b13b"
   },
   "outputs": [],
   "source": [
    "grid_params = {'n_estimators': np.arange(50, 110, 10),\n",
    "               'max_depth': np.arange(9, 16),\n",
    "               'criterion': ['squared_error', 'absolute_error']}\n",
    "\n",
    "grid = GridSearchCV(RandomForestRegressor(n_jobs=6), grid_params, cv=3, verbose=1, error_score='raise', n_jobs=2).fit(features_train, target_train)\n",
    "\n",
    "model_cv = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc3014a",
   "metadata": {
    "id": "bfc3014a",
    "outputId": "7adf0f73-bb0d-4446-b0b1-d76a9559d113"
   },
   "outputs": [],
   "source": [
    "predictions = model_cv.predict(features_test)\n",
    "print(np.sqrt(mean_squared_error(target_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7bf994",
   "metadata": {
    "id": "fb7bf994",
    "outputId": "d58deb42-1dc7-4a1a-ee9a-955c6ceccd39"
   },
   "outputs": [],
   "source": [
    "data_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b97844",
   "metadata": {
    "id": "e4b97844",
    "outputId": "ded38205-21f5-4673-fc38-79eab570d8e0"
   },
   "outputs": [],
   "source": [
    "def fill_nan(row):\n",
    "    if np.isnan(row['agr_expert']):\n",
    "        a = model_svc.predict([[row['fraction'], row['pros'], row['cons']]])[0]\n",
    "        row['agr_expert'] = a\n",
    "    elif np.isnan(row['fraction']):\n",
    "        b = np.round((model_cv.predict([[row['first'], row['second'], row['third'], row['agr_expert']]]))[0], 6)\n",
    "        row['fraction'] = b\n",
    "\n",
    "    return row\n",
    "\n",
    "data_merged = data_merged.apply(fill_nan, axis=1)\n",
    "data_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff374f0a",
   "metadata": {
    "id": "ff374f0a",
    "outputId": "75d2cc5d-873b-425b-87ad-71b0754a3eb8"
   },
   "outputs": [],
   "source": [
    "data_merged.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb662d7f",
   "metadata": {
    "id": "eb662d7f",
    "outputId": "1e3e719b-0221-4369-82ef-394f2e740926"
   },
   "outputs": [],
   "source": [
    "for i in range(data_merged.shape[0]):\n",
    "    data_merged.iloc[i, data_merged.columns.get_loc('agr_expert')] = round((data_merged.iloc[i]['agr_expert'] - 1) / 3.0, 2)\n",
    "\n",
    "data_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04776736",
   "metadata": {
    "id": "04776736",
    "outputId": "963953cb-590b-4f0b-c61a-041c0a732bfd"
   },
   "outputs": [],
   "source": [
    "data_merged.agr_expert.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7168139",
   "metadata": {
    "id": "b7168139",
    "outputId": "770e7bbd-f02d-46fc-9ff7-404b156a416d"
   },
   "outputs": [],
   "source": [
    "data_merged.dropna(axis=1, inplace=True)\n",
    "data_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd09de9f",
   "metadata": {
    "id": "fd09de9f",
    "outputId": "74a292aa-4cb4-4764-fb90-1ceb18d8ac41"
   },
   "outputs": [],
   "source": [
    "def scoring(row):\n",
    "    row['score'] = row['agr_expert'] * 0.75 + row['fraction']*0.25\n",
    "    return row\n",
    "\n",
    "data_merged = data_merged.apply(scoring, axis=1)\n",
    "display(data_merged)\n",
    "display(data_merged.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c7963",
   "metadata": {
    "id": "a10c7963"
   },
   "outputs": [],
   "source": [
    "train_data = pd.merge(train_data, data_merged[['image', 'query_id', 'score']], how='outer', on=['image', 'query_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d018b",
   "metadata": {
    "id": "1d4d018b",
    "outputId": "1cfd9049-c492-4b88-e9fa-ef40066ca74d"
   },
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa829c88",
   "metadata": {
    "id": "aa829c88",
    "outputId": "1b2b43da-d33e-416b-af63-156acfcac872",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "notna_train = train_data[train_data['query_text'].notna()]\n",
    "notna_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9795ce",
   "metadata": {
    "id": "ca9795ce",
    "outputId": "d9d306ab-f744-43f2-ac58-0f0f3f8c731c"
   },
   "outputs": [],
   "source": [
    "idx = 5855\n",
    "print(train_data.iloc[idx])\n",
    "qid = train_data.iloc[idx]['query_id']\n",
    "notna_train.loc[notna_train['query_id'] == qid]['query_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e41d6c",
   "metadata": {
    "id": "d1e41d6c",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def fillna_train(row):\n",
    "\n",
    "    if pd.isnull(row['query_text']):\n",
    "        text = notna_train[notna_train['query_id'] == row['query_id']]['query_text']\n",
    "        if len(text) != 0:\n",
    "            row['query_text'] = text.iloc[0]\n",
    "    return row\n",
    "\n",
    "train_data = train_data.apply(fillna_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2426ae9",
   "metadata": {
    "id": "a2426ae9",
    "outputId": "92b86888-f3de-490b-d871-dade5fc9e3d3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc0334d",
   "metadata": {
    "id": "4cc0334d",
    "outputId": "4cc60078-1873-4292-878d-7ed9d0d78f65"
   },
   "outputs": [],
   "source": [
    "train_data.dropna(inplace=True)\n",
    "display(train_data.head(10))\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b15a2f",
   "metadata": {
    "id": "f5b15a2f",
    "outputId": "cd7e01fe-56d7-4e68-9a9d-2b7001cd7aab"
   },
   "outputs": [],
   "source": [
    "train_data.iloc[5682]['query_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727f8308",
   "metadata": {
    "id": "727f8308"
   },
   "outputs": [],
   "source": [
    "l = WordNetLemmatizer()\n",
    "stop_words = nltk_stopwords.words(\"english\")\n",
    "def lemmatize_text(phrase):\n",
    "\n",
    "    phrase = re.sub('[^a-zA-Z]', ' ', phrase).lower()\n",
    "    phrase = nltk.word_tokenize(phrase, language = 'english')\n",
    "    phrase = [l.lemmatize(i, pos=\"n\") for i in phrase]\n",
    "    phrase = [l.lemmatize(i, pos=\"v\") for i in phrase]\n",
    "    phrase = [l.lemmatize(i, pos=\"a\") for i in phrase]\n",
    "    phrase = [l.lemmatize(i, pos=\"r\") for i in phrase]\n",
    "    phrase = [l.lemmatize(i, pos=\"s\") for i in phrase]\n",
    "    phrase = \" \".join([i for i in phrase if i not in stop_words])\n",
    "    return phrase\n",
    "\n",
    "\n",
    "def blocking(row):\n",
    "\n",
    "    phrase = lemmatize_text(row['query_text'])\n",
    "    bin_array = [i for i in phrase.split() if i in WORD_TO_BLOCK]\n",
    "    if bin_array:\n",
    "        row['word_to_block'] = 1\n",
    "    else:\n",
    "        row['word_to_block'] = 0\n",
    "\n",
    "    return row\n",
    "\n",
    "train_data = train_data.apply(blocking, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c914dda",
   "metadata": {
    "id": "6c914dda",
    "outputId": "e3f222a6-cefd-4147-f30f-8ceea43b2edd"
   },
   "outputs": [],
   "source": [
    "train_data = train_data.loc[train_data['word_to_block'] == 0].reset_index(drop=True)\n",
    "train_data.drop(columns=['word_to_block'], inplace=True)\n",
    "display(train_data.head())\n",
    "train_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdd941e",
   "metadata": {
    "id": "0cdd941e"
   },
   "outputs": [],
   "source": [
    "train_data = train_data.reset_index(drop=True)\n",
    "copy_train_data = train_data.copy()\n",
    "copy_test_data = test_query.copy()\n",
    "copy_test_data[\"score\"] = 1\n",
    "copy_train_data , copy_val_data = train_test_split(copy_train_data, test_size=0.33, random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25692c7",
   "metadata": {
    "id": "e25692c7"
   },
   "outputs": [],
   "source": [
    "random_state = 12345\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba58a1",
   "metadata": {
    "id": "64ba58a1"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4173c8",
   "metadata": {
    "id": "2a4173c8"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b631f113",
   "metadata": {
    "id": "b631f113"
   },
   "outputs": [],
   "source": [
    "copy_train_data[\"query_text\"] = copy_train_data[\"query_text\"].apply(lambda x: lemmatize_text(x))\n",
    "copy_test_data[\"query_text\"] = copy_test_data[\"query_text\"].apply(lambda x: lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c11c7d",
   "metadata": {
    "id": "83c11c7d"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(copy_train_data[\"query_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4381fc",
   "metadata": {
    "id": "5c4381fc"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, path=None, train=True):\n",
    "        self.train = train\n",
    "\n",
    "        if self.train:\n",
    "            self.images = data[\"image\"]\n",
    "            self.description = data[\"query_text\"]\n",
    "            self.target = data[\"score\"]\n",
    "            self.transform = transform\n",
    "            self.path = path\n",
    "\n",
    "        else:\n",
    "            self.description = data[\"query_text\"]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.images)\n",
    "        else:\n",
    "            return len(self.description)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.train:\n",
    "            img_name = self.images.iloc[idx]\n",
    "            image = Image.open(f\"{self.path}{img_name}\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                tokens = torch.tensor(pad_sequences(tokenizer.texts_to_sequences([self.description.iloc[idx]]), maxlen=20)[0]).to(device)\n",
    "                target = torch.tensor(1 if self.target.iloc[idx] > 0.8 else -1, dtype=torch.float32).to(device)\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image).to(device)\n",
    "\n",
    "            return image, tokens, target\n",
    "\n",
    "        else:\n",
    "            tokens = torch.tensor(pad_sequences(tokenizer.texts_to_sequences([self.description.iloc[idx]]), maxlen=20)[0]).to(device)\n",
    "            return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05388295",
   "metadata": {
    "id": "05388295"
   },
   "outputs": [],
   "source": [
    "train_data_set = CustomDataset(copy_train_data, \"to_upload/train_images/\")\n",
    "train_data_loader = DataLoader(train_data_set, batch_size=250, shuffle=True)\n",
    "test_data_set = CustomDataset(copy_test_data, \"to_upload/test_images/\")\n",
    "test_data_loader = DataLoader(test_data_set, batch_size=500, shuffle=False)\n",
    "val_data_set = CustomDataset(copy_val_data, \"to_upload/train_images/\")\n",
    "val_data_loader = DataLoader(val_data_set, batch_size=250, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081f3214",
   "metadata": {
    "id": "081f3214"
   },
   "outputs": [],
   "source": [
    "vocub_size = len(tokenizer.word_counts) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c5cc3a",
   "metadata": {
    "id": "14c5cc3a"
   },
   "outputs": [],
   "source": [
    "class LSTM_1 (nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM_1, self).__init__()\n",
    "\n",
    "        self.embbedings =nn.Embedding(vocub_size, 1024)\n",
    "        self.lstm = nn.LSTM(input_size=1024, hidden_size=512, num_layers=5, batch_first=True)\n",
    "\n",
    "    def forward(self, x) -> torch.tensor:\n",
    "        x = self.embbedings(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        return x[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3552ff",
   "metadata": {
    "id": "5f3552ff"
   },
   "outputs": [],
   "source": [
    "class Conv_1(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Conv_1, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=24, kernel_size=3)\n",
    "        self.max1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=24, out_channels=16, kernel_size=3)\n",
    "        self.max2 = nn.MaxPool2d(2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3)\n",
    "        self.max3 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(5408, (5408 // 3) * 2)\n",
    "        self.fc2 = nn.Linear((5408 // 3) * 2, 5408 // 3)\n",
    "        self.fc3 = nn.Linear(5408 // 3, 512)\n",
    "\n",
    "    def forward(self, x) -> torch.tensor:\n",
    "        x = self.conv1(x)\n",
    "        x = self.max1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.max2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.max3(x)\n",
    "        x = self.fc1(x.flatten(1))\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987485bf",
   "metadata": {
    "id": "987485bf"
   },
   "outputs": [],
   "source": [
    "conv_1 = Conv_1().to(device)\n",
    "lstm_1 = LSTM_1().to(device)\n",
    "optimazer_conv_1 = torch.optim.Adam(conv_1.parameters(), lr=1e-4)\n",
    "optimazer_lstm_1 = torch.optim.Adam(lstm_1.parameters(), lr=1e-4)\n",
    "citeration = nn.CosineEmbeddingLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5422fc4f",
   "metadata": {
    "id": "5422fc4f",
    "outputId": "749c07d1-25b1-4916-f963-08cba6d33891"
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for _ in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    eval_loss = 0\n",
    "    conv_1.train()\n",
    "    lstm_1.train()\n",
    "\n",
    "    for image, texts, target in train_data_loader:\n",
    "        optimazer_conv_1.zero_grad()\n",
    "        optimazer_lstm_1.zero_grad()\n",
    "\n",
    "        lstm_outputs = lstm_1(texts)\n",
    "        cnn_outputs = conv_1(image)\n",
    "\n",
    "        loss = citeration(cnn_outputs, lstm_outputs, target)\n",
    "        loss.backward()\n",
    "\n",
    "        optimazer_conv_1.step()\n",
    "        optimazer_lstm_1.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    conv_1.eval()\n",
    "    lstm_1.eval()\n",
    "\n",
    "    for image, texts, target in val_data_loader:\n",
    "        with torch.no_grad():\n",
    "            lstm_outputs = lstm_1(texts)\n",
    "            cnn_outputs = conv_1(image)\n",
    "\n",
    "        loss = citeration(cnn_outputs, lstm_outputs, target)\n",
    "\n",
    "        eval_loss += loss.item()\n",
    "\n",
    "    avarage_eval_loss = eval_loss / len(val_data_loader)\n",
    "    average_loss = total_loss / len(train_data)\n",
    "    print(f'Epoch [{_+1}/{num_epochs}], Loss: {average_loss:.4f}, Eval: {avarage_eval_loss:.4f}')\n",
    "\n",
    "conv_1.eval()\n",
    "lstm_1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b4767d",
   "metadata": {
    "id": "72b4767d"
   },
   "outputs": [],
   "source": [
    "for images, words, targets in test_data_loader:\n",
    "    results_images = conv_1(images).to(\"cpu\").detach().numpy()\n",
    "    results_sequences = lstm_1(words).to(\"cpu\").detach().numpy()\n",
    "    resulted_data_frame_images = pd.DataFrame(results_images)\n",
    "\n",
    "resulted_data_frame_images =  resulted_data_frame_images.drop_duplicates()\n",
    "resulted_indexies_images = list(resulted_data_frame_images.index)\n",
    "resulted_data_frame_images =  resulted_data_frame_images.reset_index(drop=True)\n",
    "resulted_data_frame_images[\"image\"] = copy_test_data.iloc[resulted_indexies_images][\"image\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa69711",
   "metadata": {
    "id": "1fa69711"
   },
   "outputs": [],
   "source": [
    "def test(text:[str], text_model):\n",
    "    title = text[0]\n",
    "    text = pd.DataFrame(text, columns=[\"query_text\"])\n",
    "    text[\"query_text\"] = text[\"query_text\"].apply(lambda x: lemmatize_text(x))\n",
    "    text = CustomDataset(text, train=False)\n",
    "    text = iter(DataLoader(text, batch_size=1))\n",
    "    text = text_model(next(text)).to(\"cpu\").detach().numpy()[0].reshape(1, -1)\n",
    "\n",
    "    similarity = []\n",
    "\n",
    "    data = resulted_data_frame_images.copy()\n",
    "\n",
    "    for i in data.drop(\"image\", axis=1).to_numpy():\n",
    "        i = i.reshape(1, -1)\n",
    "        sim = cosine_similarity(text, i)[0, 0]\n",
    "        similarity.append(sim)\n",
    "\n",
    "    data[\"similarity\"] = similarity\n",
    "    data = data.sort_values(\"similarity\").reset_index(drop=True)\n",
    "    images = data[\"image\"].tail(5).to_list()\n",
    "    similarity = data[\"similarity\"].tail(5).to_list()\n",
    "\n",
    "    fig, axs = plt.subplots(4, 4, figsize=(10, 10))\n",
    "    fig.suptitle(title)\n",
    "    min_len = 5\n",
    "\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        if i < min_len:\n",
    "            image = Image.open(f'to_upload/test_images/{images[i]}')\n",
    "            ax.imshow(image)\n",
    "            ax.set_title(f\"{similarity[i]:0.4f}\")\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax.axis('off')\n",
    "\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd3a3c5",
   "metadata": {
    "id": "7fd3a3c5",
    "outputId": "30703bb9-018e-4307-d26a-fc68c5bf1d96"
   },
   "outputs": [],
   "source": [
    "test_query[\"query_text\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa856437",
   "metadata": {
    "id": "aa856437",
    "outputId": "de4afe54-a10b-4d86-feeb-91ee98f8fafa"
   },
   "outputs": [],
   "source": [
    "test([\"The woman lacrosse player in blue is about to catch the ball .\"], lstm_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e87fcf4",
   "metadata": {
    "id": "8e87fcf4",
    "outputId": "4dbfb994-efe6-45f1-ab70-ce06ad37dd24"
   },
   "outputs": [],
   "source": [
    "test([\"A sad looking dog sitting next to shrubs\"], lstm_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96771ab6",
   "metadata": {
    "id": "96771ab6",
    "outputId": "26722016-391b-48e5-d9df-a853ce2a7001"
   },
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf13d09a",
   "metadata": {
    "id": "cf13d09a",
    "outputId": "28eaaa05-0522-404c-d871-bf58d49d0ce9"
   },
   "outputs": [],
   "source": [
    "for i in range(16):\n",
    "    text = test_query.loc[i][\"query_text\"]\n",
    "    for g in WORD_TO_BLOCK:\n",
    "            if g in lemmatize_text(text):\n",
    "                print(\"This image is not avalible\")\n",
    "                break\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            image = Image.open(f\"to_upload/test_images/{test_query.loc[i]['image']}\")\n",
    "            inputs = processor(text=[text], images=image, return_tensors=\"pt\", padding=True)\n",
    "            outputs = model(**inputs)\n",
    "            logits_per_image = outputs.logits_per_image\n",
    "            probs = logits_per_image.softmax(dim=1)\n",
    "            _class = int(probs[0][0])\n",
    "\n",
    "            plt.title(f\"Title: {text}\\nClass: {_class}\")\n",
    "\n",
    "            plt.imshow(image)\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
