{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"Markdown base-markdown base-markdown_with-gallery markdown markdown_size_normal markdown_type_theory full-markdown\"><h1>Проект для «Викишоп»</h1><div class=\"paragraph\">Поздравляем! Вы прошли курс в тренажёре. Самое время проверить знания и решить новую задачу машинного обучения. Выполнять работу будете самостоятельно.  </div><div class=\"paragraph\">Когда закончите, отправьте её на проверку ревьюеру: он пришлёт комментарии в течение суток. После этого нужно доработать проект и пройти повторную проверку. </div><div class=\"paragraph\">Скорее всего, вы доработаете кейс по комментариям ещё несколько раз. Это нормально. </div><div class=\"paragraph\">Проект завершён, когда ревьюер одобрит все доработки.</div><h2>Описание проекта</h2><div class=\"paragraph\">Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. </div><div class=\"paragraph\">Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.</div><div class=\"paragraph\">Постройте модель со значением метрики качества <em>F1</em> не меньше 0.75. </div><h3>Инструкция для проекта</h3><div class=\"paragraph\">Решить задачу можно как с помощью BERT, так и без этой нейронки. Если хотите попробовать BERT —</div><ul><li>Выполните проект локально. В тренажере тетрадь Jupyter ограничена 4 ГБ оперативной памяти — для проекта с BERT этого может не хватить.</li><li>Упомяните BERT в заголовке проекта в первой ячейке:</li></ul><div class=\"paragraph\"><div class=\"downloadable-image\"><a class=\"downloadable-image__button\" download=\"Image.png\"><svg class=\"icon icon-arrows-24-download downloadable-image__icon\" width=\"24\" height=\"24\" viewBox=\"0 0 24 24\" fill=\"none\"><path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M12 3C11.45 3 11 3.45 11 4V10.9219C11 11.6763 11.0854 12.4276 11.254 13.1613L11.0483 13.3684L10.8331 13.0242C10.4323 12.3835 9.96022 11.7902 9.42583 11.2558L8.46 10.29C8.07 9.89999 7.44 9.89999 7.05 10.29C6.66 10.68 6.66 11.32 7.05 11.71L10.9404 15.5926C11.526 16.1769 12.474 16.1769 13.0596 15.5926L16.95 11.71C17.34 11.32 17.34 10.68 16.95 10.29C16.56 9.89999 15.93 9.89999 15.54 10.29L14.5742 11.2558C14.0398 11.7902 13.5677 12.3835 13.1669 13.0242L12.9517 13.3684L12.746 13.1613C12.9146 12.4276 13 11.6763 13 10.9219V4C13 3.45 12.55 3 12 3ZM7 19C6.44772 19 6 19.4477 6 20C6 20.5523 6.44772 21 7 21H17C17.5523 21 18 20.5523 18 20C18 19.4477 17.5523 19 17 19H7Z\" fill=\"currentColor\" fill-opacity=\"0.85\"></path></svg></a><img src=\"https://pictures.s3.yandex.net:443/resources/Untitled_55_1610551674.png\" alt=\"image\" crossorigin=\"anonymous\" class=\"image image_expandable\"></div></div><div class=\"paragraph\">Выполнить проект без BERT можно локально или в нашем тренажёре.</div><div class=\"paragraph\">В любом случае алгоритм решения выглядит так:</div><ol start=\"1\"><li>Загрузите и подготовьте данные.</li><li>Обучите разные модели.</li><li>Сделайте выводы.</li></ol><h3>Описание данных</h3><div class=\"paragraph\">Данные находятся в файле <code class=\"code-inline code-inline_theme_light\">/datasets/toxic_comments.csv</code>. <a href=\"https://code.s3.yandex.net/datasets/toxic_comments.csv\" target=\"_blank\">Скачать датасет</a>. </div><div class=\"paragraph\">Столбец <em>text</em> в нём содержит текст комментария, а <em>toxic</em> — целевой признак.</div></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/datasets/toxic_comments.csv\").drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words = [\"\".join(re.sub(r'[^\\w\\s]', ' ', i).lower().split()) for i in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(string:str) -> str:\n",
    "    string = re.sub(r'[^a-zA-Z]', ' ', string).lower()\n",
    "    string = re.sub(r'\\b(\\w+)(?:\\s+\\1\\b)+', '', string)\n",
    "    string = re.sub(r'\\b(\\w*(\\w)\\2\\w*)\\b|\\b(\\w+)\\b(?:\\s+\\3\\b)+', '', string).split()\n",
    "    stems = [lemmatizer.lemmatize(i, pos=\"n\") for i in string]\n",
    "    stems = [lemmatizer.lemmatize(i, pos=\"v\") for i in stems]\n",
    "    stems = [lemmatizer.lemmatize(i, pos=\"a\") for i in stems]\n",
    "    stems = [lemmatizer.lemmatize(i, pos=\"r\") for i in stems]\n",
    "    stems = [lemmatizer.lemmatize(i, pos=\"s\") for i in stems]\n",
    "    stems = \" \".join([i for i in stems if i not in stop_words])\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "data[\"text\"] = data[\"text\"].progress_apply(lambda x: tokenizer(x))\n",
    "                                  \n",
    "nans = []\n",
    "for i, v in enumerate(data[\"text\"]):\n",
    "    if len(v) == 0:\n",
    "        nans.append(i)\n",
    "\n",
    "data = data.drop(nans, axis=0).reset_index(drop=True)\n",
    "\n",
    "features = data[\"text\"]\n",
    "target = data[\"toxic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(features, target, random_state=12345, stratify=target, test_size=0.33)\n",
    "features_test, features_val, target_test, target_val = train_test_split(features_test, target_test, random_state=12345, stratify=target_test, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vecs = vectorizer.fit_transform(features_train)\n",
    "unique_words = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = pd.DataFrame(features_train).reset_index(drop=True)\n",
    "features_test = pd.DataFrame(features_test).reset_index(drop=True)\n",
    "features_val = pd.DataFrame(features_val).reset_index(drop=True)\n",
    "target_train = pd.DataFrame(target_train).reset_index(drop=True)\n",
    "target_test = pd.DataFrame(target_test).reset_index(drop=True)\n",
    "target_val = pd.DataFrame(target_val).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_tokenizer = {k:v for v, k in enumerate(unique_words)}\n",
    "dict_tokenizer[\"unknown\"] = len(dict_tokenizer)\n",
    "vocab_size = len(dict_tokenizer) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(string:str) -> torch.tensor:\n",
    "    lbd = lambda x: dict_tokenizer[x] if x in dict_tokenizer else dict_tokenizer[\"unknown\"]\n",
    "    string = [lbd(i) for i in string.split()]\n",
    "    return torch.tensor(string, dtype=torch.int32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train[\"text\"] = features_train[\"text\"].apply(lambda x: tokenization(x))\n",
    "features_test[\"text\"] = features_test[\"text\"].apply(lambda x: tokenization(x))\n",
    "features_val[\"text\"] = features_val[\"text\"].apply(lambda x: tokenization(x))\n",
    "target_train[\"toxic\"] = target_train[\"toxic\"].apply(lambda x: torch.tensor([x], dtype=torch.float32).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embeddings_len):\n",
    "        super(RNN_LSTM, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embeddings_len)\n",
    "        self.lstm = nn.LSTM(input_size=embeddings_len, hidden_size=embeddings_len//2, num_layers=5, batch_first=True)\n",
    "        self.fc_in = nn.Linear(embeddings_len//2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.embeddings(x)\n",
    "        out, _ = self.lstm(out)\n",
    "        out = self.fc_in(out[-1])\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = RNN_LSTM(vocab_size, 256).to(device)\n",
    "citeration = nn.BCELoss().to(device)\n",
    "optimazer = Adam(model_lstm.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "for _ in range(num_epochs):\n",
    "    model_lstm.train()\n",
    "    total_loss = 0\n",
    "    for features, target in tqdm(zip(features_train[\"text\"], target_train[\"toxic\"])):\n",
    "        optimazer.zero_grad()\n",
    "        input = model_lstm(features)\n",
    "        loss = citeration(input, target)\n",
    "        loss.backward()\n",
    "        optimazer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(features_train)\n",
    "    print(f'Epoch [{_+1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "\n",
    "model_lstm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    predictions = []\n",
    "    treash_hold = g\n",
    "\n",
    "    for i in tqdm(features_val[\"text\"]):\n",
    "        prediction = model_lstm(i)\n",
    "        prediction = 1 if prediction >= treash_hold else 0\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    acc_lstm = accuracy_score(target_val[\"toxic\"], predictions)\n",
    "    f1_lstm = f1_score(target_val[\"toxic\"], predictions)\n",
    "    precision_lstm = precision_score(target_val[\"toxic\"], predictions)\n",
    "    recall_lstm = recall_score(target_val[\"toxic\"], predictions)\n",
    "\n",
    "    print(f\"\"\"Accuracy: {acc_lstm:.4f}\n",
    "    F1: {f1_lstm:.4f}\n",
    "    Precision: {precision_lstm:.4f}\n",
    "    Recall: {recall_lstm:.4f}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_GRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embeddings_len):\n",
    "        super(RNN_GRU, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embeddings_len)\n",
    "        self.lstm = nn.GRU(input_size=embeddings_len, hidden_size=embeddings_len//2, num_layers=5, batch_first=True)\n",
    "        self.fc_in = nn.Linear(embeddings_len//2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.embeddings(x)\n",
    "        out, _ = self.lstm(out)\n",
    "        out = self.fc_in(out[-1])\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru = RNN_GRU(vocab_size, 256).to(device)\n",
    "citeration = nn.BCELoss().to(device)\n",
    "optimazer = Adam(model_gru.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "for _ in range(num_epochs):\n",
    "    model_gru.train()\n",
    "    total_loss = 0\n",
    "    for features, target in tqdm(zip(features_train[\"text\"], target_train[\"toxic\"])):\n",
    "        optimazer.zero_grad()\n",
    "        input = model_gru(features)\n",
    "        loss = citeration(input, target)\n",
    "        loss.backward()\n",
    "        optimazer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(features_train)\n",
    "    print(f'Epoch [{_+1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "model_gru.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    predictions = []\n",
    "    treash_hold = g\n",
    "\n",
    "    for i in tqdm(features_val[\"text\"]):\n",
    "        prediction = model_gru(i)\n",
    "        prediction = 1 if prediction >= treash_hold else 0\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    acc_gru = accuracy_score(target_val[\"toxic\"], predictions)\n",
    "    f1_gru = f1_score(target_val[\"toxic\"], predictions)\n",
    "    precision_gru = precision_score(target_val[\"toxic\"], predictions)\n",
    "    recall_gru = recall_score(target_val[\"toxic\"], predictions)\n",
    "\n",
    "    print(f\"\"\"Accuracy: {acc_gru:.4f}\n",
    "    F1: {f1_gru:.4f}\n",
    "    Precision: {precision_gru:.4f}\n",
    "    Recall: {recall_gru:.4f}\n",
    "    Treash_hold: {treash_hold:.4f}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "treash_hold = 0.48\n",
    "\n",
    "for i in tqdm(features_test[\"text\"]):\n",
    "    prediction = model_lstm(i)\n",
    "    prediction = 1 if prediction >= treash_hold else 0\n",
    "    predictions.append(prediction)\n",
    "\n",
    "acc_lstm = accuracy_score(target_test[\"toxic\"], predictions)\n",
    "f1_lstm = f1_score(target_test[\"toxic\"], predictions)\n",
    "precision_lstm = precision_score(target_test[\"toxic\"], predictions)\n",
    "recall_lstm = recall_score(target_test[\"toxic\"], predictions)\n",
    "\n",
    "print(f\"\"\"Accuracy: {acc_lstm:.2f}\n",
    "F1: {f1_lstm:.2f}\n",
    "Precision: {precision_lstm:.2f}\n",
    "Recall: {recall_lstm:.2f}\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
