{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/datasets/toxic_comments.csv\").drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words(\"english\")\n",
    "stop_words = [\"\".join(re.sub(r'[^\\w\\s]', ' ', i).lower().split()) for i in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(string:str) -> str:\n",
    "    string = re.sub(r'[^a-zA-Z]', ' ', string).lower()\n",
    "    string = re.sub(r'\\b(\\w+)(?:\\s+\\1\\b)+', '', string)\n",
    "    string = re.sub(r'\\b(\\w*(\\w)\\2\\w*)\\b|\\b(\\w+)\\b(?:\\s+\\3\\b)+', '', string).split()\n",
    "    stems = [lemmatizer.lemmatize(i, pos=\"n\") for i in string]\n",
    "    stems = [lemmatizer.lemmatize(i, pos=\"v\") for i in stems]\n",
    "    stems = [lemmatizer.lemmatize(i, pos=\"a\") for i in stems]\n",
    "    stems = [lemmatizer.lemmatize(i, pos=\"r\") for i in stems]\n",
    "    stems = [lemmatizer.lemmatize(i, pos=\"s\") for i in stems]\n",
    "    stems = \" \".join([i for i in stems if i not in stop_words])\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "data[\"text\"] = data[\"text\"].progress_apply(lambda x: tokenizer(x))\n",
    "                                  \n",
    "nans = []\n",
    "for i, v in enumerate(data[\"text\"]):\n",
    "    if len(v) == 0:\n",
    "        nans.append(i)\n",
    "\n",
    "data = data.drop(nans, axis=0).reset_index(drop=True)\n",
    "\n",
    "features = data[\"text\"]\n",
    "target = data[\"toxic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, target_train, target_test = train_test_split(features, target, random_state=12345, stratify=target, test_size=0.33)\n",
    "features_test, features_val, target_test, target_val = train_test_split(features_test, target_test, random_state=12345, stratify=target_test, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vecs = vectorizer.fit_transform(features_train)\n",
    "unique_words = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = pd.DataFrame(features_train).reset_index(drop=True)\n",
    "features_test = pd.DataFrame(features_test).reset_index(drop=True)\n",
    "features_val = pd.DataFrame(features_val).reset_index(drop=True)\n",
    "target_train = pd.DataFrame(target_train).reset_index(drop=True)\n",
    "target_test = pd.DataFrame(target_test).reset_index(drop=True)\n",
    "target_val = pd.DataFrame(target_val).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_tokenizer = {k:v for v, k in enumerate(unique_words)}\n",
    "dict_tokenizer[\"unknown\"] = len(dict_tokenizer)\n",
    "vocab_size = len(dict_tokenizer) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(string:str) -> torch.tensor:\n",
    "    lbd = lambda x: dict_tokenizer[x] if x in dict_tokenizer else dict_tokenizer[\"unknown\"]\n",
    "    string = [lbd(i) for i in string.split()]\n",
    "    return torch.tensor(string, dtype=torch.int32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train[\"text\"] = features_train[\"text\"].apply(lambda x: tokenization(x))\n",
    "features_test[\"text\"] = features_test[\"text\"].apply(lambda x: tokenization(x))\n",
    "features_val[\"text\"] = features_val[\"text\"].apply(lambda x: tokenization(x))\n",
    "target_train[\"toxic\"] = target_train[\"toxic\"].apply(lambda x: torch.tensor([x], dtype=torch.float32).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embeddings_len):\n",
    "        super(RNN_LSTM, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embeddings_len)\n",
    "        self.lstm = nn.LSTM(input_size=embeddings_len, hidden_size=embeddings_len//2, num_layers=5, batch_first=True)\n",
    "        self.fc_in = nn.Linear(embeddings_len//2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.embeddings(x)\n",
    "        out, _ = self.lstm(out)\n",
    "        out = self.fc_in(out[-1])\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = RNN_LSTM(vocab_size, 256).to(device)\n",
    "citeration = nn.BCELoss().to(device)\n",
    "optimazer = Adam(model_lstm.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "for _ in range(num_epochs):\n",
    "    model_lstm.train()\n",
    "    total_loss = 0\n",
    "    for features, target in tqdm(zip(features_train[\"text\"], target_train[\"toxic\"])):\n",
    "        optimazer.zero_grad()\n",
    "        input = model_lstm(features)\n",
    "        loss = citeration(input, target)\n",
    "        loss.backward()\n",
    "        optimazer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(features_train)\n",
    "    print(f'Epoch [{_+1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "\n",
    "model_lstm.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    predictions = []\n",
    "    treash_hold = g\n",
    "\n",
    "    for i in tqdm(features_val[\"text\"]):\n",
    "        prediction = model_lstm(i)\n",
    "        prediction = 1 if prediction >= treash_hold else 0\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    acc_lstm = accuracy_score(target_val[\"toxic\"], predictions)\n",
    "    f1_lstm = f1_score(target_val[\"toxic\"], predictions)\n",
    "    precision_lstm = precision_score(target_val[\"toxic\"], predictions)\n",
    "    recall_lstm = recall_score(target_val[\"toxic\"], predictions)\n",
    "\n",
    "    print(f\"\"\"Accuracy: {acc_lstm:.4f}\n",
    "    F1: {f1_lstm:.4f}\n",
    "    Precision: {precision_lstm:.4f}\n",
    "    Recall: {recall_lstm:.4f}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_GRU(nn.Module):\n",
    "    def __init__(self, vocab_size, embeddings_len):\n",
    "        super(RNN_GRU, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embeddings_len)\n",
    "        self.lstm = nn.GRU(input_size=embeddings_len, hidden_size=embeddings_len//2, num_layers=5, batch_first=True)\n",
    "        self.fc_in = nn.Linear(embeddings_len//2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.embeddings(x)\n",
    "        out, _ = self.lstm(out)\n",
    "        out = self.fc_in(out[-1])\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru = RNN_GRU(vocab_size, 256).to(device)\n",
    "citeration = nn.BCELoss().to(device)\n",
    "optimazer = Adam(model_gru.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "\n",
    "for _ in range(num_epochs):\n",
    "    model_gru.train()\n",
    "    total_loss = 0\n",
    "    for features, target in tqdm(zip(features_train[\"text\"], target_train[\"toxic\"])):\n",
    "        optimazer.zero_grad()\n",
    "        input = model_gru(features)\n",
    "        loss = citeration(input, target)\n",
    "        loss.backward()\n",
    "        optimazer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(features_train)\n",
    "    print(f'Epoch [{_+1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
    "model_gru.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    predictions = []\n",
    "    treash_hold = g\n",
    "\n",
    "    for i in tqdm(features_val[\"text\"]):\n",
    "        prediction = model_gru(i)\n",
    "        prediction = 1 if prediction >= treash_hold else 0\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    acc_gru = accuracy_score(target_val[\"toxic\"], predictions)\n",
    "    f1_gru = f1_score(target_val[\"toxic\"], predictions)\n",
    "    precision_gru = precision_score(target_val[\"toxic\"], predictions)\n",
    "    recall_gru = recall_score(target_val[\"toxic\"], predictions)\n",
    "\n",
    "    print(f\"\"\"Accuracy: {acc_gru:.4f}\n",
    "    F1: {f1_gru:.4f}\n",
    "    Precision: {precision_gru:.4f}\n",
    "    Recall: {recall_gru:.4f}\n",
    "    Treash_hold: {treash_hold:.4f}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "treash_hold = 0.48\n",
    "\n",
    "for i in tqdm(features_test[\"text\"]):\n",
    "    prediction = model_lstm(i)\n",
    "    prediction = 1 if prediction >= treash_hold else 0\n",
    "    predictions.append(prediction)\n",
    "\n",
    "acc_lstm = accuracy_score(target_test[\"toxic\"], predictions)\n",
    "f1_lstm = f1_score(target_test[\"toxic\"], predictions)\n",
    "precision_lstm = precision_score(target_test[\"toxic\"], predictions)\n",
    "recall_lstm = recall_score(target_test[\"toxic\"], predictions)\n",
    "\n",
    "print(f\"\"\"Accuracy: {acc_lstm:.2f}\n",
    "F1: {f1_lstm:.2f}\n",
    "Precision: {precision_lstm:.2f}\n",
    "Recall: {recall_lstm:.2f}\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
